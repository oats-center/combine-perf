{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6931934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:33.648586Z",
     "start_time": "2021-06-06T21:18:32.510040Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import fiona\n",
    "\n",
    "import alphashape as ashp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import colorcet as cc\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import LineString, Polygon, Point, MultiPoint\n",
    "from shapely.ops import transform, unary_union\n",
    "from shapely import affinity\n",
    "from descartes import PolygonPatch\n",
    "from scipy.optimize import minimize\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from matplotlib import colors\n",
    "from pyproj import Proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26de4d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:33.652550Z",
     "start_time": "2021-06-06T21:18:33.650031Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8be62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:33.657427Z",
     "start_time": "2021-06-06T21:18:33.654118Z"
    }
   },
   "outputs": [],
   "source": [
    "SM_H5_PATH = '/home/yang/output/eswe/smooth/'\n",
    "CLS_H5_PATH = '/home/yang/output/eswe/cls/'\n",
    "OUTPUT_PATH = '/home/yang/output/eswe/opt/'\n",
    "EVENT = 'jkwh'\n",
    "MACHINE_IDS = ['2388', '6088']\n",
    "DATE = '072019'\n",
    "FIELD_ID = 'f8'\n",
    "sm_log_names = ['-'.join([EVENT, mid, DATE, FIELD_ID, 'gps-sm.h5']) for mid in MACHINE_IDS]\n",
    "cls_log_names = ['-'.join([EVENT, mid, DATE, FIELD_ID, 'gps-cls.h5']) for mid in MACHINE_IDS]\n",
    "track_colors = ['r', 'royalblue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f17d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:33.660422Z",
     "start_time": "2021-06-06T21:18:33.658696Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Set up dimensions in ft\n",
    "# w_ft = [30, 32, 35]\n",
    "# # Convert these to meters\n",
    "# w_m = [w*0.3048 for w in w_ft]\n",
    "# # Create dimension combos\n",
    "# w_comb = [(w0, w1) for w0 in w_m for w1 in w_m]\n",
    "w_comb = (9.754,9.754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac247d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.271976Z",
     "start_time": "2021-06-06T21:18:33.661467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataframes\n",
    "gpss = {}\n",
    "for mid, n1, n2 in zip(MACHINE_IDS, sm_log_names, cls_log_names):\n",
    "    # Concatenate each machine's smoothed and classified data\n",
    "    df_sm = pd.read_hdf(SM_H5_PATH + n1)\n",
    "    df_cls = pd.read_hdf(CLS_H5_PATH + n2)\n",
    "    df_concat = pd.concat([df_cls, df_sm[['xs', 'ys']]], axis=1)\n",
    "    # Just like in the previous two steps, we want to group the points because there might be gaps\n",
    "    # Compute time difference\n",
    "    gps_seg = df_concat.copy()\n",
    "    gps_seg['ts_diff'] = gps_seg['ts'].diff()\n",
    "    gps_seg.loc[0, 'ts_diff'] = 0\n",
    "    # A trick vector that could be used for grouping\n",
    "    gps_seg['ts_diff_binary'] = gps_seg['ts_diff'] > 10\n",
    "    gps_seg['ts_diff_binary_cumsum'] = gps_seg['ts_diff_binary'].apply(lambda x: 1 if x else 0).cumsum()\n",
    "    # Points groups\n",
    "    grp = gps_seg.groupby('ts_diff_binary_cumsum')\n",
    "    # Make indices right\n",
    "    gps_seg['seg_num'] = grp.ngroup()\n",
    "    gps_seg.loc[0, 'seg_num'] = 0\n",
    "    gps_seg = gps_seg.drop(columns=['ts_diff', 'ts_diff_binary', 'ts_diff_binary_cumsum'])\n",
    "    # Add it to a dict for easy query\n",
    "    gpss[mid] = gps_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16426be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.278592Z",
     "start_time": "2021-06-06T21:18:34.274215Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/59903415/how-do-i-calculate-the-angle-between-two-points\n",
    "def get_track(r):\n",
    "    delta_x = r['xs'] - r['xss']\n",
    "    delta_y = r['ys'] - r['yss']\n",
    "    deg_r = np.arctan2(delta_y, delta_x)\n",
    "    deg_d = deg_r * 180 / np.pi - 90\n",
    "    deg_true_north = (deg_d + 360) % 360\n",
    "    return deg_true_north"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce04b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.284107Z",
     "start_time": "2021-06-06T21:18:34.280911Z"
    }
   },
   "outputs": [],
   "source": [
    "d0 = gpss[MACHINE_IDS[0]]\n",
    "d1 = gpss[MACHINE_IDS[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79c1b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.296626Z",
     "start_time": "2021-06-06T21:18:34.287054Z"
    }
   },
   "outputs": [],
   "source": [
    "d0 = d0.drop(columns=['lat', 'lon', 'alt', 'cv_prob', 'nct_prob'])\n",
    "d1 = d1.drop(columns=['lat', 'lon', 'alt', 'cv_prob', 'nct_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea140fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.312393Z",
     "start_time": "2021-06-06T21:18:34.299168Z"
    }
   },
   "outputs": [],
   "source": [
    "pts_grps0 = [g[1] for g in d0.groupby('seg_num')]\n",
    "pts_grps1 = [g[1] for g in d1.groupby('seg_num')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbe749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.342301Z",
     "start_time": "2021-06-06T21:18:34.314020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the boundary\n",
    "fss = gpd.read_file('/home/yang/data/raw/jkwh.kml', driver='KML')\n",
    "fss.head()\n",
    "field_name = FIELD_ID\n",
    "fs = fss[fss['Name'] == field_name].copy()\n",
    "fs = fs.reset_index(drop=True)\n",
    "proj = Proj(proj='utm', zone=13, ellps='WGS84', preserve_units=False)\n",
    "fs_poly = transform(lambda x, y, z: proj(x, y), fs['geometry'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da31098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.603244Z",
     "start_time": "2021-06-06T21:18:34.343963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the tracks and field polygons\n",
    "# %matplotlib notebook\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "d0.plot.scatter(x='xs', y='ys', s=1, c='r', ax=ax)\n",
    "d1.plot.scatter(x='xs', y='ys', s=1, c='royalblue', ax=ax)\n",
    "ax.add_patch(PolygonPatch(fs_poly, fc='none', ec='magenta', ls='--'))\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756d2ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.607201Z",
     "start_time": "2021-06-06T21:18:34.604924Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the figure, look at which track corresponds to the outer shape of the field polygon\n",
    "# OUTER_INDEX = 1\n",
    "# if OUTER_INDEX == 0:\n",
    "#     pts = d0.apply(lambda r: Point(r['xs'], r['ys']), axis=1)\n",
    "#     fs_ashp = ashp.alphashape(pts, 0.01)\n",
    "# else:\n",
    "#     pts = d1.apply(lambda r: Point(r['xs'], r['ys']), axis=1)\n",
    "#     fs_ashp = ashp.alphashape(pts, 0.01)\n",
    "# # Create field polygon collections\n",
    "# fs_ashp_f = {}\n",
    "# fs_ashp_f = Polygon(fs_ashp.exterior.parallel_offset(w_comb[OUTER_INDEX] / 2, side='left', join_style=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396259a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.612164Z",
     "start_time": "2021-06-06T21:18:34.608838Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_data_chunks(pts_grps, n=500):\n",
    "    data_chunks = []\n",
    "    # Go through the pts_grps\n",
    "    for pts_grp in pts_grps:\n",
    "        # If the num. of data points is less than 10000, we don't do anything\n",
    "        if len(pts_grp) < n:\n",
    "            data_chunks.append(pts_grp)\n",
    "        else: # Otherwise we will break it apart into n (default: 500) num. row data chunks\n",
    "            m = 0\n",
    "            for g, df in pts_grp.groupby(np.arange(len(pts_grp)) // n):\n",
    "#                 if m > 0:\n",
    "#                     # We don't want discontinued path so that we will be missing polygons,\n",
    "#                     # so we will generate length-2 chunks to piece all the chunks together\n",
    "#                     data_chunks.append(pd.concat([data_chunks[-1].iloc[-1], df.iloc[0]], axis=1).transpose())\n",
    "                data_chunks.append(df)\n",
    "                m += 1\n",
    "    return data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c05125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.631432Z",
     "start_time": "2021-06-06T21:18:34.613839Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_shps(pts_grp, params):\n",
    "    xoff, yoff, w = params\n",
    "    # Make an copy of the points data\n",
    "#     df = pts_grp[pts_grp['mode'] != 'nw-p'].copy()\n",
    "#     df = df.reset_index(drop=True)\n",
    "    df = pts_grp.copy()\n",
    "    # Create a column for *geospatially* shifted points\n",
    "    df['geom_pt_s'] = np.nan\n",
    "    # Sanity check on the length of the dataframe\n",
    "#     print(len(df))\n",
    "    if len(df) <= 2:\n",
    "#         print('Only one point in group, this should never happen')\n",
    "#         print('len <= 1')\n",
    "        return df\n",
    "    # Shift coordinates up by one timestep\n",
    "    df['xs_s'] = df['xs'].shift(-1)\n",
    "    df['ys_s'] = df['ys'].shift(-1)\n",
    "    # Make sure every point has a shifted coordinate\n",
    "    df.loc[df.index[-1], 'xs_s'] = df.loc[df.index[-2], 'xs_s'] + 0.01\n",
    "    df.loc[df.index[-1], 'ys_s'] = df.loc[df.index[-2], 'ys_s'] + 0.01\n",
    "    # Create gdf from original points for processing\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['xs'], df['ys']))\n",
    "    # Rename the geometry to avoid confusion\n",
    "    gdf.rename_geometry('geom_pt_o', inplace=True)\n",
    "    # Get the freebies (original points) from the gdf\n",
    "    df['geom_pt_o'] = gdf['geom_pt_o']\n",
    "    # Create linestring from original points to time-shifted points\n",
    "    gdf['geom_ls_o2s'] = gdf.apply(lambda r: LineString([r['geom_pt_o'], Point(r['xs_s'], r['ys_s'])]), axis=1)\n",
    "#     # HACK: if the linestring is too short, drop these rows\n",
    "    gdf['geom_ls_o2s_len'] = gdf.apply(lambda r: r['geom_ls_o2s'].length, axis=1)\n",
    "    gdf.loc[gdf['geom_ls_o2s_len'] < 1e-2, 'geom_ls_o2s_len'] = 1e-2\n",
    "    # Scale `ls_o2s` to extend it to the right length\n",
    "    # https://shapely.readthedocs.io/en/latest/manual.html#shapely.affinity.scale\n",
    "    gdf['geom_ls_o2s_scaled'] = gdf.apply(lambda r: affinity.scale(r['geom_ls_o2s'], \\\n",
    "        xfact=abs(yoff)/r['geom_ls_o2s_len'], yfact=abs(yoff)/r['geom_ls_o2s_len'], origin=r['geom_pt_o']), axis=1)\n",
    "    # Handle four cases\n",
    "    if (xoff == 0) & (yoff == 0):\n",
    "        # Both offsets are 0, no shift geospatially\n",
    "        df['geom_pt_s'] = df['geom_pt_o']\n",
    "    if (xoff != 0) & (yoff != 0):\n",
    "        # Both offsets not 0, need to scale, and then parallel shift\n",
    "        # Then we need to parallel shift the scaled linestring\n",
    "        if xoff < 0:\n",
    "            offset_dir = 'left'\n",
    "            b_idx = 0\n",
    "            r_idx = 1\n",
    "        else:\n",
    "            offset_dir = 'right'\n",
    "            b_idx = 1\n",
    "            r_idx = 0\n",
    "        gdf['geom_ls_o2s_scaled_po'] = gdf.apply(lambda r: \\\n",
    "            r['geom_ls_o2s_scaled'].parallel_offset(abs(xoff), offset_dir) if r['geom_ls_o2s_scaled'].is_valid else None, axis=1)\n",
    "        # Depending on the sign of yoff, we need to rotate the parallel shifted linestring\n",
    "        if yoff > 0:\n",
    "            rotation_angle = 360\n",
    "        else:\n",
    "            rotation_angle = 180\n",
    "        # Rotate the line\n",
    "        gdf['geom_ls_o2s_scaled_po_rot'] = gdf.apply(lambda r: affinity.rotate(r['geom_ls_o2s_scaled_po'], \\\n",
    "            rotation_angle, origin=r['geom_ls_o2s_scaled_po'].boundary[b_idx]) if r['geom_ls_o2s_scaled_po'] != None else None, axis=1)\n",
    "        # Obtain the coordinate\n",
    "        df['geom_pt_s'] = gdf.apply(lambda r: r['geom_ls_o2s_scaled_po_rot'].boundary[r_idx] if r['geom_ls_o2s_scaled_po_rot'] != None else None, axis=1)\n",
    "    if (xoff == 0) & (yoff != 0):\n",
    "        # Depending on the sign of yoff, we need to rotate the parallel shifted linestring\n",
    "        if yoff > 0:\n",
    "            rotation_angle = 360\n",
    "        else:\n",
    "            rotation_angle = 180\n",
    "        r_idx = 1\n",
    "        # Rotate the line\n",
    "        gdf['geom_ls_o2s_scaled_rot'] = gdf.apply(lambda r: \\\n",
    "            affinity.rotate(r['geom_ls_o2s_scaled'], rotation_angle, origin=r['geom_pt_o']) if r['geom_ls_o2s_scaled'].is_valid else None, axis=1)\n",
    "        # xoff is 0, so we need to get the right linestring by scaling the linestring, then get the endpoint with no parallel shift\n",
    "        df['geom_pt_s'] = gdf.apply(lambda r: r['geom_ls_o2s_scaled_rot'].boundary[r_idx] if r['geom_ls_o2s_scaled_rot'] != None else None, axis=1)\n",
    "    if (xoff != 0) & (yoff == 0):\n",
    "        # yoff is 0, so we need the get the linestring, no scaling, just parallel shift\n",
    "        if xoff < 0:\n",
    "            offset_dir = 'left'\n",
    "            b_idx = 0\n",
    "        else:\n",
    "            offset_dir = 'right'\n",
    "            b_idx = 1\n",
    "        gdf['geom_ls_o2s_scaled_po'] = gdf.apply(lambda r: \\\n",
    "            r['geom_ls_o2s'].parallel_offset(abs(xoff), offset_dir) if r['geom_ls_o2s'].is_valid else None, axis=1)\n",
    "        df['geom_pt_s'] = gdf.apply(lambda r: r['geom_ls_o2s_scaled_po'].boundary[b_idx] if r['geom_ls_o2s_scaled_po'] != None else None, axis=1)\n",
    "            \n",
    "    # Let's generate the shapes now\n",
    "    df_shp = df.copy()\n",
    "    df_shp = df_shp.dropna(subset=['geom_pt_s']).reset_index(drop=True)\n",
    "    # Shift the points up by one timestep\n",
    "    df_shp['geom_pt_s_s'] = df_shp['geom_pt_s'].shift(-1)\n",
    "    # Create linestrings by connecting the paths\n",
    "    df_shp = df_shp.iloc[:-2].copy()\n",
    "    df_shp['geom_ls_path'] = df_shp.apply(lambda r: LineString([r['geom_pt_s'], r['geom_pt_s_s']]) if (r['geom_pt_s'] != None) & (r['geom_pt_s_s'] != None) else None, axis=1)\n",
    "    # Extend the each path to left/right to obtain header edge points\n",
    "    geom_ls_path_gs = gpd.GeoSeries(df_shp['geom_ls_path'])\n",
    "    df_shp['geom_pt_edge_l'] = geom_ls_path_gs.apply(lambda r: r.parallel_offset(w / 2, 'left').boundary[1] if (r.length > 1e-5) else np.nan)\n",
    "    df_shp['geom_pt_edge_r'] = geom_ls_path_gs.apply(lambda r: r.parallel_offset(w / 2, 'right').boundary[0] if (r.length > 1e-5) else np.nan)\n",
    "    # Create dataframe to line these points up\n",
    "    df_edge_pts = pd.concat([df_shp['geom_pt_edge_l'].rename('l'), df_shp['geom_pt_edge_l'].shift(-1).rename('ls'), \\\n",
    "                             df_shp['geom_pt_edge_r'].shift(-1).rename('rs'), df_shp['geom_pt_edge_r'].rename('r')], axis=1)\n",
    "    # Drop the last one since it will be nan\n",
    "    df_edge_pts = df_edge_pts.drop(index=[df_edge_pts.index[-1]])\n",
    "    # Sometimes this happens and we don't have any polygons\n",
    "    if len(df_edge_pts) == 0:\n",
    "        return df_shp\n",
    "    # Create these polygons and swath lines\n",
    "    df_shp['geom_poly_cvg'] = df_edge_pts.apply(lambda r: Polygon([r['l'], r['ls'], r['rs'], r['r']]).convex_hull \\\n",
    "                                                if (r['l'] != None) & (r['ls'] != None) & (r['rs'] != None) & (r['r'] != None) else None, axis=1)\n",
    "    df_shp['geom_ls_swath'] = df_edge_pts.apply(lambda r: LineString([r['l'], r['r']]) if (r['l'] != None) & (r['r'] != None) else None, axis=1)\n",
    "    # Check discontiuities\n",
    "#     poly_cvg = gpd.GeoSeries(df_shp['geom_poly_cvg'])\n",
    "#     df_shp = df_shp.drop(index=poly_cvg[poly_cvg.area > 50].index).reset_index(drop=True)\n",
    "        \n",
    "    return df_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b3f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.634611Z",
     "start_time": "2021-06-06T21:18:34.632968Z"
    }
   },
   "outputs": [],
   "source": [
    "# [len(p) for p in pts_grps0]\n",
    "# df_shp = gen_shps(pts_grps0[5], [0,0,w_comb[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440c4cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.637214Z",
     "start_time": "2021-06-06T21:18:34.635697Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain coverage polygons and swath linestrings\n",
    "# p = Pool(15)\n",
    "# res0 = [p.apply_async(gen_shps, args=(g, [0,0,w_comb[0]])) for g in pts_grps0]\n",
    "# p.close()\n",
    "# p.join()\n",
    "# p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c64a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.639789Z",
     "start_time": "2021-06-06T21:18:34.638291Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Concatenate all the dataframes within the group\n",
    "# df = pd.concat([r.get() for r in res0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bc1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.642327Z",
     "start_time": "2021-06-06T21:18:34.640860Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_shp_o = gen_shps(pts_grps1[2], [0,0,9.144])\n",
    "# df_shp = gen_shps(pts_grps1[0], [0,8,9.144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0decb83b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.644978Z",
     "start_time": "2021-06-06T21:18:34.643397Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# polys_cvg_o = gpd.GeoSeries(df_shp_o.loc[:, 'geom_poly_cvg'])\n",
    "# polys_cvg = gpd.GeoSeries(df_shp.loc[:, 'geom_poly_cvg'])\n",
    "# polys_cvg.plot(alpha=0.8, color='red', ax=ax)\n",
    "# polys_cvg_o.plot(alpha=0.4, color='royalblue', ax=ax)\n",
    "# ax.add_patch(PolygonPatch(fs_poly, fc='none', ec='cyan', ls='--'))\n",
    "# ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc36e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.664819Z",
     "start_time": "2021-06-06T21:18:34.646091Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_shps_new(pts_grp, params):\n",
    "    xoff, yoff, w = params\n",
    "    df = pts_grp.copy()\n",
    "    df['geom_pt_o'] = np.nan # column for offsetted points\n",
    "    # If we have only we point in the grp, then just return it with a ring buffer\n",
    "    if len(df) == 1:\n",
    "        print('This should never happen, only one point in group')\n",
    "        df['geom_poly_cvg'] = Point(df['xs'].values, df['ys'].values).buffer(w)\n",
    "        return df\n",
    "    # Shift coordinates up by one time step\n",
    "    df['xss'] = df['xs'].shift(-1)\n",
    "    df['yss'] = df['ys'].shift(-1)\n",
    "    # Hack: make sure every point has a coordinate\n",
    "    df.loc[df.index[-1], 'xss'] = df.loc[df.index[-2], 'xss'] + 0.01\n",
    "    df.loc[df.index[-1], 'yss'] = df.loc[df.index[-2], 'yss'] + 0.01\n",
    "    # Create gdf from original points for processing\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['xs'], df['ys']))\n",
    "    # Rename the geometry to avoid confusion\n",
    "    gdf.rename_geometry('geom_pt_s', inplace=True)\n",
    "    # Get the original points\n",
    "    df['geom_pt_s'] = gdf['geom_pt_s']\n",
    "    # Put xs, ys shifted into gdf\n",
    "    gdf['geom_pt_ss'] = gdf.apply(lambda r: Point(r['xss'],r['yss']), axis=1)\n",
    "    # Create linestring from pt_s to pt_ss\n",
    "    gdf['geom_ls_s2ss'] = gdf.apply(lambda r: LineString([r['geom_pt_s'], r['geom_pt_ss']]), axis=1)\n",
    "    # Create point buffer from geom_pt_s\n",
    "    # xoff is 0 but yoff is not, we simply create the buffer but do not compute the parallel offset\n",
    "    if (xoff == 0) & (yoff != 0):\n",
    "        print('xoff == 0, yoff != 0')\n",
    "        # When xoff is 0, we only need yoff\n",
    "        # Method: 1) create a circle buffer and use it to intersect with the linestring connecting pt(t) and pt(t+1)\n",
    "        #         2) if yoff > 0, the endpoint of the intersection is what we want (rotation by 360) doesn't change the coordinate\n",
    "        #            if yoff < 0, the intersected linestring should be rotated 180 to obtain the endpoint\n",
    "        if yoff > 0:\n",
    "            rotation_angle = 360\n",
    "        else:\n",
    "            rotation_angle = 180\n",
    "        gdf['geom_ls_a'] = gdf.apply(lambda r: r['geom_pt_s'].buffer(abs(yoff)).intersection(r['geom_ls_s2ss']), axis=1)\n",
    "        rotated_ls_a = gpd.GeoSeries(gdf.apply(lambda r: affinity.rotate(r['geom_ls_a'], rotation_angle, origin=r['geom_pt_s']), axis=1))\n",
    "        df['geom_pt_o'] = rotated_ls_a.apply(lambda r: r.boundary[1])\n",
    "    elif (xoff != 0) & (yoff == 0):\n",
    "#         print('xoff != 0, yoff == 0')\n",
    "        # When yoff is 0, we only need xoff\n",
    "        # See method when xoff == 0 and yoff != 0\n",
    "        if xoff > 0:\n",
    "            rotation_angle = -90\n",
    "        else:\n",
    "            rotation_angle = 90\n",
    "        gdf['geom_ls_a'] = gdf.apply(lambda r: r['geom_pt_s'].buffer(abs(xoff)).intersection(r['geom_ls_s2ss']), axis=1)\n",
    "        rotated_ls_a = gpd.GeoSeries(gdf.apply(lambda r: affinity.rotate(r['geom_ls_a'], rotation_angle, origin=r['geom_pt_s']), axis=1))\n",
    "        df['geom_pt_o'] = rotated_ls_a.apply(lambda r: r.boundary[1])\n",
    "    elif (xoff == 0) & (yoff == 0):\n",
    "#         print('xoff == 0, yoff == 0')\n",
    "        df['geom_pt_o'] = df['geom_pt_s']\n",
    "    else:\n",
    "#         print('xoff != 0, yoff != 0')\n",
    "        gdf['geom_ls_a'] = gdf.apply(lambda r: r['geom_pt_s'].buffer(abs(yoff)).intersection(r['geom_ls_s2ss']), axis=1)\n",
    "        # Get the intersection linestring and rotate it based on yoff > 0 or < 0\n",
    "        if yoff > 0:\n",
    "            rotation_angle = 360\n",
    "            rotated = False\n",
    "        else:\n",
    "            rotation_angle = 180\n",
    "            rotated = True\n",
    "        rotated_ls_a = gpd.GeoSeries(gdf.apply(lambda r: affinity.rotate(r['geom_ls_a'], rotation_angle, origin=r['geom_pt_s']), axis=1))\n",
    "        \n",
    "        if (rotated) & (xoff > 0):\n",
    "            offset_dir = 'left'\n",
    "            bidx = 1\n",
    "        elif (rotated) & (xoff < 0):\n",
    "            offset_dir = 'right'\n",
    "            bidx = 0\n",
    "        elif (~rotated) & (xoff > 0):\n",
    "            offset_dir = 'right'\n",
    "            bidx = 0\n",
    "        elif (~rotated) & (xoff < 0):\n",
    "            offset_dir = 'left'\n",
    "            bidx = 1\n",
    "            \n",
    "        gdf['geom_ls_a_po'] = rotated_ls_a.apply(lambda r: r.parallel_offset(abs(xoff), offset_dir))\n",
    "        df['geom_pt_o'] = gdf.apply(lambda r: r['geom_ls_a_po'].boundary[bidx], axis=1)\n",
    "    \n",
    "    # Let's generate the shapes now\n",
    "    df_shp = df.copy()\n",
    "    df_shp['geom_pt_o_s'] = df_shp['geom_pt_o'].shift(-1)\n",
    "    # Last point will be null\n",
    "    if len(df_shp) == 2:\n",
    "        df_shp = df_shp.iloc[:-1].copy()\n",
    "    else:\n",
    "        df_shp = df_shp.iloc[:-2].copy()\n",
    "    # Check if there is any nan\n",
    "    nan_idx = df_shp[df_shp['geom_pt_o_s'].isna()].index.values\n",
    "    if len(nan_idx) != 0:\n",
    "        # This should not happen since nan's should have been already removed in prior steps\n",
    "        return -1\n",
    "    # Create path linestrings\n",
    "    df_shp['geom_ls_path'] = df_shp.apply(lambda r: LineString([r['geom_pt_o'], r['geom_pt_o_s']]), axis=1)\n",
    "    # When we only have two points, at this point there will be only one linestring created\n",
    "    # So we will need to connect pt(t+1) to pt(t) to create a linestring for properly creating the polygon\n",
    "    if len(df_shp) == 1:\n",
    "#         print('length is 1')\n",
    "        df_shp.loc[df_shp.index[0]+1, 'geom_ls_path'] = LineString([df_shp.loc[df_shp.index[0], 'geom_pt_o_s'], df_shp.loc[df_shp.index[0], 'geom_pt_s']])\n",
    "    \n",
    "    # Check if any path has distance 0\n",
    "    ls_path_zero_idx = df_shp[df_shp.apply(lambda r: r['geom_ls_path'].distance == 0, axis=1)].index\n",
    "    if len(ls_path_zero_idx) != 0:\n",
    "        # This should not happen either\n",
    "        return -1\n",
    "    # Check if any path is None\n",
    "    ls_path_nan_idx = df_shp[df_shp['geom_ls_path'] == None].index\n",
    "    if len(ls_path_nan_idx) != 0:\n",
    "        # This should not happen either\n",
    "        return -1\n",
    "    # Create left and right edge points by extending 1/2 swath width\n",
    "    try:\n",
    "        geom_ls_path_gs = gpd.GeoSeries(df_shp['geom_ls_path'])\n",
    "#         df_shp['geom_pt_edge_l'] = df_shp.apply(lambda r: r['geom_ls_path'].parallel_offset(w / 2, 'left').boundary[1], axis=1)\n",
    "#         df_shp['geom_pt_edge_r'] = df_shp.apply(lambda r: r['geom_ls_path'].parallel_offset(w / 2, 'right').boundary[0], axis=1)\n",
    "        df_shp['geom_pt_edge_l'] = geom_ls_path_gs.apply(lambda r: r.parallel_offset(w / 2, 'left').boundary[1] if (r.length > 1e-5) else np.nan)\n",
    "        df_shp['geom_pt_edge_r'] = geom_ls_path_gs.apply(lambda r: r.parallel_offset(w / 2, 'right').boundary[0] if (r.length > 1e-5) else np.nan)\n",
    "    except IndexError as e:\n",
    "        print('IndexError!!!! {}, {}, {}, {}, {}'.format(xoff, yoff, w, len(df_shp), sum(df_shp['geom_ls_path'] == None)))\n",
    "        return df_shp\n",
    "    # Create dataframe to line these points up\n",
    "    df_edge_pts = pd.concat([df_shp['geom_pt_edge_l'].rename('l'), df_shp['geom_pt_edge_l'].shift(-1).rename('ls'), \\\n",
    "                             df_shp['geom_pt_edge_r'].shift(-1).rename('rs'), df_shp['geom_pt_edge_r'].rename('r')], axis=1)\n",
    "    \n",
    "    # Drop the last one since it will be nan\n",
    "    df_edge_pts = df_edge_pts.drop(index=[df_edge_pts.index[-1]])\n",
    "    # Drop any nan columns\n",
    "    df_edge_pts = df_edge_pts.dropna()\n",
    "    # Create these polygons and swath lines\n",
    "    df_shp['geom_poly_cvg'] = gpd.GeoSeries(df_edge_pts.apply(lambda r: Polygon([r['l'], r['ls'], r['rs'], r['r']]).convex_hull, axis=1))\n",
    "    df_shp['geom_ls_swath'] = gpd.GeoSeries(df_edge_pts.apply(lambda r: LineString([r['l'], r['r']]), axis=1))\n",
    "    \n",
    "    # Drop the last row when creating polygon using only 2 points\n",
    "    if len(df_shp) == 2:\n",
    "        df_shp = df_shp.drop(index=[df_shp.index[-1]])\n",
    "    \n",
    "    return df_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd441dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.671669Z",
     "start_time": "2021-06-06T21:18:34.665920Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(X_to_opt, args):\n",
    "    # Obtain the arguments\n",
    "    xoff0, yoff0, xoff1, yoff1 = X_to_opt\n",
    "    p, w0, w1, m0_grps, m1_grps = args\n",
    "    # Obtain the async results\n",
    "    res0 = [p.apply_async(gen_shps, args=(g, [xoff0,yoff0,w0])) for g in m0_grps]\n",
    "    res1 = [p.apply_async(gen_shps, args=(g, [xoff1,yoff1,w1])) for g in m1_grps]\n",
    "    # Concatenate all the dataframes within the group\n",
    "    df_shp0 = pd.concat([r.get() for r in res0])\n",
    "    df_shp1 = pd.concat([r.get() for r in res1])\n",
    "    # Get only the working polygons\n",
    "#     poly_cvg0 = gpd.GeoSeries(df_shp0.loc[df_shp0['mode'] == 'w', 'geom_poly_cvg'])\n",
    "#     poly_cvg1 = gpd.GeoSeries(df_shp1.loc[df_shp1['mode'] == 'w', 'geom_poly_cvg'])\n",
    "    poly_cvg0 = gpd.GeoSeries(df_shp0.loc[:, 'geom_poly_cvg'])\n",
    "    poly_cvg1 = gpd.GeoSeries(df_shp1.loc[:, 'geom_poly_cvg'])\n",
    "    # Compute the total covered area with overlap\n",
    "    area_covered_w_overlap = poly_cvg0.area.sum() + poly_cvg1.area.sum()\n",
    "    # Obtain the covered area polygon\n",
    "    poly_cvg_union = unary_union([poly_cvg0.unary_union, poly_cvg1.unary_union])\n",
    "    poly_cvg_actual = fs_poly.intersection(poly_cvg_union)\n",
    "#     poly_cvg_actual = fs_ashp_f.intersection(poly_cvg_union)\n",
    "    # Get the covered area\n",
    "    area_covered_actual = poly_cvg_union.area\n",
    "    # Compute the overlap area\n",
    "    area_overlap = area_covered_w_overlap - area_covered_actual\n",
    "    # Get the skipped area\n",
    "    poly_skip = fs_poly.difference(poly_cvg_actual)\n",
    "#     poly_skip = fs_ashp_f.difference(poly_cvg_actual)\n",
    "    print('xoff{}: {:.2f} | yoff{}: {:.2f} | xoff{}: {:.2f} | yoff{}: {:.2f} | w{}: {:.2f} | w{}: {:.2f}\\na_o: {:.2f} | a_s: {:.2f} | a_to: {:.2f} | a_a: {:.2f} | a_fs: {:.2f}'.format(\\\n",
    "        MACHINE_IDS[0], xoff0, MACHINE_IDS[0], yoff0, MACHINE_IDS[1], xoff1, MACHINE_IDS[1], yoff1, \\\n",
    "        MACHINE_IDS[0], w0, MACHINE_IDS[1], w1, area_overlap, poly_skip.area, area_covered_w_overlap, area_covered_actual, fs_poly.area))\n",
    "    print('\\tcost: {:.2f}'.format(1/2*area_overlap+1/2*poly_skip.area))\n",
    "    return 1/2*area_overlap+1/2*poly_skip.area\n",
    "#     print('\\tcost: {:.2f}'.format(-area_covered_actual))\n",
    "#     return poly_skip.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be4712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.674537Z",
     "start_time": "2021-06-06T21:18:34.672787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Before optimization, let's do a few test runs\n",
    "# p = Pool(15)\n",
    "# xoff0 = 0\n",
    "# yoff0 = 0\n",
    "# xoff1 = 0\n",
    "# yoff1 = 0\n",
    "# for w_tuple in w_comb:\n",
    "#     print(w_tuple)\n",
    "#     res = f([xoff0,yoff0,xoff1,yoff1], [p,w_tuple[0],w_tuple[1],pts_grps0,pts_grps1])\n",
    "# p.close()\n",
    "# p.join()\n",
    "# p.terminate()\n",
    "\n",
    "# # Get results for both machines\n",
    "# res0 = [p.apply_async(gen_shps_new, args=(g, [xoff0,yoff0,w0])) for g in pts_grps0]\n",
    "# res1 = [p.apply_async(gen_shps_new, args=(g, [xoff1,yoff1,w1])) for g in pts_grps1]\n",
    "\n",
    "# # Concatenate all the dataframes within the group\n",
    "# df_shp0 = pd.concat([r.get() for r in res0])\n",
    "# df_shp1 = pd.concat([r.get() for r in res1])\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# shp_cvg0 = gpd.GeoSeries(df_shp0.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg1 = gpd.GeoSeries(df_shp1.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg0.plot(alpha=0.6, color='r', ax=ax)\n",
    "# shp_cvg1.plot(alpha=0.6, color='blue', ax=ax)\n",
    "# ax.add_patch(PolygonPatch(fs_poly, fc='none', ec='orange', ls='--'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf888f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.693424Z",
     "start_time": "2021-06-06T21:18:34.676778Z"
    }
   },
   "outputs": [],
   "source": [
    "pts_grps0_chunks = gen_data_chunks(pts_grps0)\n",
    "pts_grps1_chunks = gen_data_chunks(pts_grps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552aeb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.696393Z",
     "start_time": "2021-06-06T21:18:34.694778Z"
    }
   },
   "outputs": [],
   "source": [
    "# for idx, c in enumerate(pts_grps1_chunks):\n",
    "#     print(idx, len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9fff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.699178Z",
     "start_time": "2021-06-06T21:18:34.697497Z"
    }
   },
   "outputs": [],
   "source": [
    "# p = Pool(15)\n",
    "# Get results for both machines\n",
    "# re = [-137234.58902515998, 0.15708930613405675, 3.1440586726632014, -0.1985936886748788, 8.47810408120722, 9.144, 9.144]\n",
    "# res0 = [p.apply_async(gen_shps, args=(g, [re[1],re[2],9.144])) for g in pts_grps0_chunks]\n",
    "# res1 = [p.apply_async(gen_shps, args=(g, [0,0,9.144])) for g in pts_grps1_chunks]\n",
    "# p.close()\n",
    "# p.join()\n",
    "# p.terminate()\n",
    "\n",
    "# # Concatenate all the dataframes within the group\n",
    "# df_shp0 = pd.concat([r.get() for r in res0])\n",
    "# df_shp1 = pd.concat([r.get() for r in res1])\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# shp_cvg0 = gpd.GeoSeries(df_shp0.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg1 = gpd.GeoSeries(df_shp1.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg0.plot(alpha=0.6, color='r', ax=ax)\n",
    "# shp_cvg1.plot(alpha=0.6, color='royalblue', ax=ax)\n",
    "# ax.add_patch(PolygonPatch(fs_poly, fc='none', ec='orange', ls='--'))\n",
    "# ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32bc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:34.701928Z",
     "start_time": "2021-06-06T21:18:34.700287Z"
    }
   },
   "outputs": [],
   "source": [
    "# p = Pool(15)\n",
    "# Get results for both machines\n",
    "# res0 = [p.apply_async(gen_shps, args=(g, [0,0,9.144])) for g in pts_grps0_chunks]\n",
    "# p.close()\n",
    "# p.join()\n",
    "# p.terminate()\n",
    "# Concatenate all the dataframes within the group\n",
    "# df_shp0 = pd.concat([r.get() for r in res0])\n",
    "# df_shp1 = pd.concat([r.get() for r in res1])\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# shp_cvg0 = gpd.GeoSeries(df_shp0.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg1 = gpd.GeoSeries(df_shp1.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg0.plot(alpha=0.6, color='r', ax=ax)\n",
    "# shp_cvg1.plot(alpha=0.6, color='royalblue', ax=ax)\n",
    "# ax.add_patch(PolygonPatch(fs_poly, fc='none', ec='orange', ls='--'))\n",
    "# ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5389e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:32:12.253709Z",
     "start_time": "2021-06-06T21:18:34.702987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run optimizer\n",
    "p = Pool(15)\n",
    "r = minimize(f, [0,0,0,0], method='Powell', options={'xtol': 1e-2, 'ftol': 1}, \\\n",
    "             bounds=((-10,10),(-10,10),(-10,10),(-10,10)), args=[p,w_comb[0],w_comb[1],pts_grps0_chunks,pts_grps1_chunks])\n",
    "p.close()\n",
    "p.join()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914dab92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:32:12.257462Z",
     "start_time": "2021-06-06T21:32:12.255106Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "results += list(r.x) + list(w_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5eec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:32:12.260988Z",
     "start_time": "2021-06-06T21:32:12.258608Z"
    }
   },
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887fdaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:32:12.263641Z",
     "start_time": "2021-06-06T21:32:12.262037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Before optimization, let's do a few test runs\n",
    "# p = Pool(15)\n",
    "# # Get results for both machines\n",
    "# res0 = [p.apply_async(gen_shps, args=(g, [0,0,w_comb[0]])) for g in pts_grps0]\n",
    "# res1 = [p.apply_async(gen_shps, args=(g, [0,0,w_comb[1]])) for g in pts_grps1]\n",
    "# # Get shifted results for both machines\n",
    "# res0_s = [p.apply_async(gen_shps, args=(g, [results[0],results[1],w_comb[0]])) for g in pts_grps0]\n",
    "# res1_s = [p.apply_async(gen_shps, args=(g, [results[2],results[3],w_comb[1]])) for g in pts_grps1]\n",
    "# # Close the pool\n",
    "# p.close()\n",
    "# p.join()\n",
    "# p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2665b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:32:12.266452Z",
     "start_time": "2021-06-06T21:32:12.264790Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Concatenate all the dataframes within the group\n",
    "# df_shp0 = pd.concat([r.get() for r in res0])\n",
    "# df_shp1 = pd.concat([r.get() for r in res1])\n",
    "# shp_cvg0 = gpd.GeoSeries(df_shp0.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg1 = gpd.GeoSeries(df_shp1.loc[:, 'geom_poly_cvg'])\n",
    "# df_shp0_s = pd.concat([r.get() for r in res0_s])\n",
    "# df_shp1_s = pd.concat([r.get() for r in res1_s])\n",
    "# shp_cvg0_s = gpd.GeoSeries(df_shp0_s.loc[:, 'geom_poly_cvg'])\n",
    "# shp_cvg1_s = gpd.GeoSeries(df_shp1_s.loc[:, 'geom_poly_cvg'])\n",
    "\n",
    "# # %matplotlib notebook\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# shp_cvg0.plot(alpha=0.6, color='r', ax=ax)\n",
    "# shp_cvg1.plot(alpha=0.6, color='r', ax=ax)\n",
    "# shp_cvg0_s.plot(alpha=0.6, color='royalblue', ax=ax)\n",
    "# shp_cvg1_s.plot(alpha=0.6, color='royalblue', ax=ax)\n",
    "# ax.add_patch(PolygonPatch(fs_poly, fc='none', ec='orange', ls='--'))\n",
    "# ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162766c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:32:12.268909Z",
     "start_time": "2021-06-06T21:32:12.267441Z"
    }
   },
   "outputs": [],
   "source": [
    "# pickle_name = '-'.join([EVENT, '-'.join(MACHINE_IDS), DATE, FIELD_ID, 'opt-res.pickle'])\n",
    "# print(pickle_name)\n",
    "# with open(OUTPUT_PATH + pickle_name, 'wb') as fh:\n",
    "#     pickle.dump(results, fh, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
